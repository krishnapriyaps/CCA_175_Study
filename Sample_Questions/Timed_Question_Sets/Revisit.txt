1---------------Problem Scenario 6 :
You have been given MySQL DB with following details.
user=retail_dba
password=cloudera
database=retail_db
table=retail_db.products
jdbc URL = jdbc:mysql://quickstart:3306/retail_db
Columns of products table : (product_id | product_category_id | product_name |
product_description | product_price | product_image )
Please accomplish following activities.
1. Copy "retaildb.products" table to hdfs in a directory p93_products
2. Now sort the products data sorted by product price per category, use productcategoryid
colunm to group by category

2---------------Problem Scenario 1 : 
You have been given bloew code snippet (do a sum of values by key), with intermediate output.
val keysWithValuesList = Array ("foo=A","foo=A","foo=A","foo=A","foo=B","bas=C","bas=D","bas=D")
val data = sc.parallelize(keysWithValuesList)
// Create Key value pair
val kv = data.map(_.split("=")).map(v => (v(0), v(1))).cache()
val initialCount=0;
val countByKey=kv.aggregateByKey(initialCount)(addToCounts,SumPartialCounts)
Now Define two functions (addToCounts, sumPartitionCounts)such which will produce following resutls:
Output1: 
countByKey.collect
res3: Array[(String, int)] = Array ((foo,5),bar,3)

import scala.collection._
val initialSet =  scala.collection.mutable.HashSet.empty[String]
val uniqueByKey = kv.aggregateByKey(initialSet)(addToSet, mergePartitionSets)

Now define two functions (addToSet, mergePartitionSets) such which will produce following results

Output2:
uniqueByKey.collect
res4: Array[(String, scala.collection.mutable.HashSet[String])] =  Array((foo, Set(A,B)), bar, Set(C,D))

3---------------Problem Scenario 3 :
There is a parent organization called "Acmeshell Group Inc' which has two child companines named QuickTech and HadoopExam. Both companies employee info is given in two separate text file as below. Please do the following activity for emplyee details

quicktech.txt
1,Alok,Hyd
2,Krish,HongKong
3,Jyothi,Mumbai
4,Athul,Bangalore
5,Ishan,Gurgaon

hadoopexam.txt
6,John,Newyork
7,alp2004
8,tellme,Mumbai
9Ganan21,Pune
10,Mukesh,Chennai

1) A. Which comd will you use to check all the available command line optionss on HDFS and B. How will you get help for inidivual commad
 
2) Create a new Empty Directory named Employee using cmd line. And Also create an empty file named in it quicktech.txt

3) Load both companies employee data in Employee Directory (How to override existing file in HDFS) 

4) Merge both employees data in a single file called MergedEmployee.txt, merged files should should have new line character at the end of each file content
 cat MergedEmployee.txt
6,John,Newyork
7,alp2004
8,tellme,Mumbai
9Ganan21,Pune
10,Mukesh,Chennai

1,Alok,Hyd
2,Krish,HongKong
3,Jyothi,Mumbai
4,Athul,Bangalore
5,Ishan,Gurgaon

4---------------Problem Scenario 14 :
You have been given MySQL DB with following details. You have been given
following product.csv file 
product.csv productID,productCode,name,quantity,price
1001,PEN,Pen Red,5000,1.23
1002,PEN,Pen Blue,8000,1.25
1003,PEN,Pen Black,2000,1.25
1004,PEC,Pencil 2B,10000,0.48
1005,PEC,Pencil 2H,8000,0.49
1006,PEC,Pencil HB,0,9999.99
Now accomplish following activities.
1 . Create a Hive ORC table using SparkSql
2 . Load this data in Hive table.
3 . Create a Hive parquet table using SparkSQL and load data in it.

5---------------Problem Scenario 15 :
You have been given MySQL DB with following details. You have been given
following product.csv file 
product.csv productID,productCode,name,quantity,price
1001,PEN,Pen Red,5000,1.23
1002,PEN,Pen Blue,8000,1.25
1003,PEN,Pen Black,2000,1.25
1004,PEC,Pencil 2B,10000,0.48
1005,PEC,Pencil 2H,8000,0.49
1006,PEC,Pencil HB,0,9999.99
Now accomplish following activities.

In Continuation of previous question, please accomplish following activities.
1. Select all the products which has product code as null
2. Select all the products, whose name starts with Pen and results should be order by Price
descending order.
3. Select all the products, whose name starts with Pen and results should be order by
Price descending order and quantity
4. Select top 2 products by price

6---------------Problem Scenario 18 :
You have been given a file named spark7/EmployeeName.csv
(id,name).
EmployeeName.csv
E01,Lokesh
E02,Bhupesh
E03,Amit
E04,Ratan
E05,Dinesh
E06,Pavan
E07,Tejas
E08,Sheela
E09,Kumar
E10,Venkat
1. Load this file from hdfs and sort it by name and save it back as (id,name) in results directory.
However, make sure while saving it should be able to write In a single file.

Step 1: Load to HDFS
 hadoop fs -put file:///home/cloudera/Study/cca_probalems/problem18/employee.csv hdfs://quickstart.cloudera:8020/user/cloudera/spark_experiments/testdata

7---------------Problem Scenario 19 :
You have been given below patient data in csv format,
patientID,name,dateOfBirth,lastVisitDate
1001,Ah Teck,1991-12-31,2012-01-20
1002,Kumar,2011-10-29,2012-09-20
1003,Ali,2011-01-30,2012-10-21
Accomplish following activities.
1 . Find all the patients whose lastVisitDate between current time and '2012-09-15'
2 . Find all the patients who born in 2011
3 . Find all the patients age
4 . List patients whose last visited more than 60 days ago
5 . Select patients 18 years old or younger

8---------------Problem Scenario 20 :
You have been given sample data as below in a file called spark15/file1.txt
3070811,1963,1096,,"US","CA",,1,
3022811,1963,1096,,"US","CA",,1,56
3033811,1963,1096,,"US","CA",,1,23

Below is the code snippet to process this tile.

val field= sc.textFile("spark15/filel.txt")
val mapper = field.map(x=> A)
mapper.map(x => x.map(x=> {B})).collect
Please fill in A and B so it can generate below final output
Array(Array(3070811,1963,109G, 0, "US", "CA", 0,1, 0)
,Array(3022811,1963,1096, 0, "US", "CA", 0,1, 56)
,Array(3033811,1963,1096, 0, "US", "CA", 0,1, 23)

9---------------Problem Scenario 21 :
You have been given belwo list in scala (name,sex,cost) for each work done.
List( ("Deeapak" , "male", 4000), ("Deepak" , "male", 2000), ("Deepika" , "female",
2000),("Deepak" , "female", 2000), ("Deepak" , "male", 1000) , ("Neeta" , "female", 2000))
Now write a Spark program to load this list as an RDD and do the sum of cost for combination of
name and sex (as key))

10---------------Problem Scenario 23 :
You have given following two files
1. Content.txt: Contain a huge text file containing space separated words.
2. Remove.txt: Ignore/filter all the words given in this file (Comma Separated).
Write a Spark program which reads the Content.txt file and load as an RDD, remove all the
words from a broadcast variables (which is loaded as an RDD of words from Remove.txt).
And count the occurrence of the each word and save it as a text file in HDFS.
Content.txt
Hello this is ABCTech.com
This is TechABY.com
Apache Spark Training
This is Spark Learning Session
Spark is faster than MapReduce
Remove.txt
Hello, is, this, the


11---------------Problem 31:
You have been given following mysql database details as well as
other info.
user=retail_dba
password=cloudera
database=retail_db
jdbc URL = jdbc:mysql://quickstart:3306/retail_db
Please accomplish below assignment.
1. Create a table in hive as below.
create table departments_hive(department_id int, department_name string);
2. Now import data from mysql table departments to this hive table. Please make sure that
data should be visible using below hive command, select" from departments_hive


12-------------
You have to run your Spark application on yarn with each executor
Maximum heap size to be 512MB and Number of processor cores to allocate on each
executor will be 1 and Your main application required three values as input arguments V1
V2 V3.
Please replace XXX, YYY, ZZZ
./bin/spark-submit -class com.hadoopexam.MyTask --master yarn-cluster--num-executors 3
--driver-memory 512m XXX YYY lib/hadoopexam.jarZZZ


